# insight-genie

## terms.py

### Summary

The code defines two lists: `terms`, which contains keywords related to actions like "read" and "listen" (with some entries commented out), and `ignore_terms`, which includes phrases or handles to be excluded, specifically related to certain podcasts and social media formats.

```py
terms = [
    "read",
    "listen",
    # "watch",
    #  "attend",
    #  "present",
    #  "complete"
]

ignore_terms = [
    "rt @",
    "@azuredevopsshow",  # podcast
    "@agileuprising",  # podcast
]

```

## prompt_test.py

### Summary

This code defines a test function, `test_get_prompt`, which checks the output of the `get_prompt` function from the `insight_genie.prompts.prompt` module. It verifies that the prompt is a string and contains the current date in ISO format.

```py
from datetime import datetime

from insight_genie.prompts.prompt import get_prompt


def test_get_prompt():
    current_date = datetime.today().date().isoformat()

    prompt = get_prompt()
    assert isinstance(prompt, str)
    assert current_date in prompt

```

## author_test.py

### Summary

This code consists of a unit test for the `get_author_description` method of the `Author` class. It uses the `unittest.mock` library to create a mock for an OpenAI client.

Key components:

- It sets up a mock OpenAI client that simulates a response containing a predefined author description.
- The test initializes an `Author` object, removes any existing test files, and then calls the `get_author_description` method.
- It asserts that the description returned is not `None`.
- Finally, it cleans up by removing any test files created during the test.

The function `remove_test_file_if_exists` checks if a file exists at a specified path and removes it if it does.

```py
import os
from unittest.mock import MagicMock, patch

from .author import Author

mock_description = "Sample Author\nThis is a sample description."


@patch("insight_genie.exporters.twitter.extraction.manager.author.build_openai_client")
def test_get_author_description(mock_build_openai_client: MagicMock):
    author = Author(title="SampleTitle", mention="@mention")
    remove_test_file_if_exists(author._get_full_data_path(author.data_file_path))

    mock_client = mock_build_openai_client.return_value
    mock_client.chat.completions.create.return_value = MagicMock(
        choices=[MagicMock(message=MagicMock(content=mock_description))]
    )

    description = author.get_author_description()

    assert description is not None
    remove_test_file_if_exists(author._get_full_data_path(author.data_file_path))


def remove_test_file_if_exists(file_path: str) -> None:
    if os.path.exists(file_path):
        os.remove(file_path)

```

## ImageDescriptionPrompt.md

### Summary

`Image Analysis: Agile Insights`

- The image may feature a diagram illustrating the principles of agile methodologies, incorporating elements such as iterative development, team collaboration, and customer feedback.
- Key terms like "Scrum," "Kanban," or "Lean" might be highlighted, showing different frameworks or practices within Agile.
- Potential sections may include roles such as Product Owner, Scrum Master, and Development Team, along with responsibilities attributed to each role.
- Important artifacts like Product Backlog, Sprint Backlog, and Increment could be represented.
- Visual elements may consist of flowcharts or process cycles exemplifying the Agile workflow.
- If artificial intelligence is depicted, it might illustrate the integration of AI tools within agile development to enhance efficiency or decision-making.
- Leadership components may suggest a focus on servant leadership or empowerment of team members, promoting a collaborative environment.

```md
Please analyze the attached image by first providing a concise identifier (one word or a short phrase) that best describes the image, **placed on the first line of your response**â€”for example:

`Audiobook Cover: The Agile Samurai`

Then, on the following lines, directly extract and summarize its professional content.
Focus on transcribing visible text, interpreting diagrams, and noting unique elements in the image, especially those related to agile methodologies, artificial intelligence, software engineering, or leadership.
Provide detailed insights based solely on the specific content conveyed in the image, avoiding general assumptions or external knowledge.
Aim to deliver as much professional information as possible to enhance understanding of the main points presented.
```

## event_group.py

### Summary

The code defines a data structure for event information using the `EventGroup` dataclass, which includes properties like title, event IDs, full text, hashtags, and media URLs. It contains two main functions:

1. **`extract_event_groups`**: Takes a dictionary of grouped tweets (with titles as keys) and aggregates them into `EventGroup` instances by calling the helper function `_aggregate_event_data`.

2. **`_aggregate_event_data`**: Aggregates event data for a given group title, extracting unique event IDs, concatenated text, sorted hashtags, media URLs, mentions, and the total favorite count from a list of `EventData` objects.

Overall, this code facilitates the organization and consolidation of tweet-related event data into structured groups.

```py
from dataclasses import dataclass
from typing import Dict, List

from .event_data import EventData
from .tweet_data import TweetData


@dataclass
class EventGroup:
    title: str
    event_ids: List[str]
    full_text: str
    hashtags: List[str]
    kcv: str
    media_urls: List[str]
    mentions: List[str]
    total_favorite_count: int
    tweets: List[TweetData]


def extract_event_groups(grouped_tweets: Dict[str, List[EventData]]) -> Dict[str, EventGroup]:
    return {title: _aggregate_event_data(title, events) for title, events in grouped_tweets.items()}


def _aggregate_event_data(group_title: str, events: List[EventData]) -> EventGroup:
    event_ids = list({event_id for event in events for event_id in event.event_ids})
    full_text = "\n".join(line for event in events for line in event.text.splitlines()[1:] if line.strip())
    hashtags = sorted({hashtag for event in events for hashtag in event.hashtags})
    kcv = next((event.kcv[0] for event in events if event.kcv), "")
    media_urls = list({media_url for event in events for media_url in event.media_urls})
    mentions = sorted({mention for event in events for mention in event.mentions})
    total_favorite_count = sum(event.favorite_count for event in events)

    return EventGroup(
        event_ids=event_ids,
        full_text=full_text,
        hashtags=hashtags,
        kcv=kcv,
        media_urls=media_urls,
        mentions=mentions,
        title=group_title,
        total_favorite_count=total_favorite_count,
        tweets=events,
    )

```

## tweet_extraction.py

### Summary

This code manages and processes Twitter data by loading tweets from a JSON file, filtering them based on specific criteria, and organizing them into a structured format.

1. **Loading Data**: The `load_twitter_data()` function reads tweet data from a JSON file.
2. **Filtering Tweets**: The `filter_tweets(tweets)` function groups tweets while eliminating replies and filtering based on certain terms and conditions (e.g., avoiding ignored terms or those starting with "@").
3. **Date Parsing**: The `parse_tweet_date(tweet_data)` function converts tweet creation timestamps to datetime objects.
4. **Building a Dictionary**: The `_build_tweet_dictionary(tweets_data)` function creates a lookup dictionary of tweets indexed by their IDs.
5. **Identifying Replies**: The `_identify_replies(tweets_data, tweet_dictionary)` function compiles a set of IDs for tweets that are replies.
6. **Appending Replies**: The `_append_replies(tweets_data, tweet_dictionary, reply_ids)` function attaches replies to their parent tweets.

The overall aim is to structure and filter tweet data for further analysis or processing in a clean and organized manner.

```py
import json
from datetime import datetime
from typing import List

from ai_assistant_manager.encoding import UTF_8

from .models.tweet_data import TweetData, build_tweet_data
from .terms import ignore_terms, terms


def load_twitter_data() -> list[dict]:
    with open("./data/twitter/tweets.json", "r", encoding=UTF_8) as file:
        return json.load(file)


def filter_tweets(tweets: List[dict]) -> List[TweetData]:
    """Group tweets by their reply structure."""
    tweets_data = sorted(map(build_tweet_data, tweets), key=parse_tweet_date)
    tweet_dictionary = _build_tweet_dictionary(tweets_data)
    reply_ids = _identify_replies(tweets_data, tweet_dictionary)
    _append_replies(tweets_data, tweet_dictionary, reply_ids)

    return [
        group
        for group in list(filter(lambda td: td.id_str not in reply_ids, tweets_data))
        if any(term in group.text.split("\n")[0] for term in terms)
        and not any(term in group.text.lower() for term in ignore_terms)
        and not group.text.startswith("@")
    ]


def parse_tweet_date(tweet_data: TweetData) -> datetime:
    """Parse the created_at date of a tweet."""
    return datetime.strptime(tweet_data.created_at, "%a %b %d %H:%M:%S +0000 %Y")


def _build_tweet_dictionary(tweets_data: List[TweetData]) -> dict:
    """Build a dictionary of tweets by their id_str."""
    return {tweet_data.id_str: tweet_data for tweet_data in tweets_data}


def _identify_replies(tweets_data: List[TweetData], tweet_dictionary: dict) -> set:
    """Identify replies and return a set of reply ids."""
    return {
        tweet_data.id_str
        for tweet_data in tweets_data
        if tweet_data.in_reply_to_id and tweet_data.in_reply_to_id in tweet_dictionary
    }


def _append_replies(tweets_data: List[TweetData], tweet_dictionary: dict, reply_ids: set) -> None:
    """Append replies to their parent tweets."""
    for tweet_data in tweets_data:
        if tweet_data.id_str in reply_ids:
            parent_tweet = tweet_dictionary[tweet_data.in_reply_to_id]
            parent_tweet.replies.append(tweet_data)

```

## events.py

### Summary

The code defines functions for processing tweets to create event data structured by book titles. It imports necessary utilities for text handling and model definitions.

1. **build_events**: Takes a list of tweets and organizes them into a dictionary where each key is a book title and the value is a list of associated `EventData` objects created from the tweets.

2. **create_event_data**: This function converts individual `TweetData` objects into `EventData` by parsing the tweet properties like creation date, title, hashtags, media URLs, mentions, and cleaned text from replies. It also checks if the tweet indicates the start of an event.

Overall, the code is designed to transform tweet data into a structured format suitable for event tracking based on book references.

```py
from dataclasses import asdict
from datetime import datetime

from .extractor import (
    clean_text,
    extract_title,
    get_all_hashtags,
    get_all_ids,
    get_all_media_urls,
    get_replies_text,
    separate_kcv_lines,
)
from .models.event_data import EventData
from .models.tweet_data import TweetData


def build_events(tweets_with_terms: list[TweetData]) -> dict[str, list[EventData]]:
    events = {
        title: [
            event_data
            for event_data in (create_event_data(details) for details in tweets_with_terms)
            if event_data.title == title
        ]
        for title in {create_event_data(details).title for details in tweets_with_terms}
    }
    return events


def create_event_data(tweet_data: TweetData) -> EventData:
    is_start = "started" in tweet_data.text.lower()
    created_at_date = datetime.strptime(tweet_data.created_at, "%a %b %d %H:%M:%S +0000 %Y").strftime("%Y%m%d")
    book_title = extract_title(tweet_data.text)
    event_ids = get_all_ids(tweet_data)
    cleaned_text = clean_text(tweet_data.text + "\n" + get_replies_text(tweet_data))
    kcv, lines = separate_kcv_lines(cleaned_text)

    return EventData(
        event_date=created_at_date,
        favorite_count=tweet_data.favorite_count,
        hashtags=get_all_hashtags(tweet_data),
        is_start=is_start,
        mentions=tweet_data.mentions,
        media_urls=get_all_media_urls(tweet_data),
        kcv=kcv,
        text=lines,
        title=book_title,
        tweet_data=asdict(tweet_data),
        event_ids=event_ids,
    )

```

## run_chat.py

### Summary

The code initializes an AI assistant application that interacts with the OpenAI API. It imports necessary components such as an AI service, chat management, environment variables, prompts, and data export functions.

Key features of the code:

- Logs the assistant's start message and exports necessary data.
- Initializes an OpenAI client and an assistant service, optionally allowing for the deletion of existing assistants.
- Gets the assistant's ID and starts a chat session.
- Accepts user input messages in a loop, sending them to the assistant and printing the responses, until the user types "exit".

The main execution block sets environment variables and handles error logging.

```py
from ai_assistant_manager.assistants.assistant_service import (
    AssistantService,
)
from ai_assistant_manager.chats.chat import Chat
from ai_assistant_manager.clients.openai_api import OpenAIClient, build_openai_client
from ai_assistant_manager.env_variables import ENV_VARIABLES, set_env_variables
from ai_assistant_manager.prompts.prompt import get_prompt
from data_exporter import PROMPT_PATH, export_data, print_response
from loguru import logger

SHOULD_DELETE_ASSISTANT = False

START_MESSAGE = """"""


def main():
    logger.info(f"Starting {ENV_VARIABLES.assistant_name}")

    export_data()

    client = OpenAIClient(build_openai_client())
    service = AssistantService(client, get_prompt(prompt_path=PROMPT_PATH))

    if SHOULD_DELETE_ASSISTANT:
        logger.info("Removing existing assistant and category files")
        service.delete_assistant()

    assistant_id = service.get_assistant_id()

    logger.info(f"Assistant ID: {assistant_id}")

    chat = Chat(
        client,
        assistant_id,
        # thread_id="abc",
    )
    chat.start()

    if START_MESSAGE:
        start_response = chat.send_user_message(START_MESSAGE)
        print_response(start_response, service.assistant_name)

    while True:
        user_message = input("\nMessage: ")

        if not user_message:
            print("Invalid user message.")
            continue
        if user_message == "exit":
            break

        chat_response = chat.send_user_message(user_message)
        print_response(chat_response, service.assistant_name)


if __name__ == "__main__":
    try:
        set_env_variables()
        main()
    except Exception as e:
        logger.info(f"Error: {e}")

```

## pyproject.toml

### Summary

This code represents a configuration file for a Python project named **"insight-genie"** using **Hatch** for packaging and management.

- **Build System**: Specifies that the project requires `hatchling` for building.
- **Project Metadata**: Includes the project name, dynamic version handling, description, license file, README file, author info, Python version requirement, and dependencies on specific packages.
- **Versioning**: Defines how to extract the version from `setup.cfg` using a regex pattern.
- **Build Targets**: Specifies the inclusion of source files for source distribution (sdist) and lists packages for wheel distribution, pointing to the `src` directory.
- **Default Environment**: Configures a virtual environment with dependencies for static type checking and testing.
- **Scripts**: Defines commands for various tasks like running a chat, building, summarizing code, and testing.
- **Static Analysis**: Points to a configuration file for the `ruff` static analysis tool.
- **Linting and Testing**: Configures linting rules and pytest options for integration tests.

In summary, this configuration sets up a Python project with specific dependencies, build processes, and testing configurations using `Hatch`.

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "insight-genie"
dynamic = ["version"]
description = "Your expert craftsman for software wisdom, agile insights, and leadership."
license = { file = "LICENSE" }
readme = "README.md"
authors = [{ name = "Justin Beall", email = "jus.beall@gmail.com" }]
requires-python = ">=3.11"
dependencies = [
    "ai-assistant-manager==2.0.0",
    "ai-code-summary==0.1.1",
    "loguru",
    "openai",
    "python-dotenv",
]

[tool.hatch.version]
path = "setup.cfg"
pattern = "version = (?P<version>\\S+)"

[tool.hatch.build.targets.sdist]
include = ["/src"]

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.hatch.envs.default]
type = "virtual"
path = ".venv"
dependencies = ["pyright", "pytest", "pytest-cov"]

[tool.hatch.envs.default.scripts]
chat = "python run_chat.py"
build = "python run_build.py"
summary = "python run_code_summary.py {args}"
test = "pytest --cache-clear --cov --cov-report lcov --cov-report term -m 'not integration'"
test-integration = "pytest --cache-clear --cov"

[tool.hatch.envs.hatch-static-analysis]
config-path = "ruff_defaults.toml"

[tool.ruff]
extend = "ruff_defaults.toml"

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "parents"

[tool.pytest.ini_options]
markers = "integration: an integration test that hits external uncontrolled systems"

```

## continuous-integration.yml

### Summary

This code is a GitHub Actions workflow named "Continuous Integration" that runs automatically on push events to any branch. It defines a job called "tests" that executes on an Ubuntu environment. The job includes the following steps:

1. Checkout the repository code.
2. Set up Python version 3.x.
3. Install project dependencies using Hatch.
4. Execute unit tests using Hatch.

Overall, it automates the process of testing Python code whenever changes are pushed to the repository.

```yml
name: Continuous Integration

on:
  push:
    branches: ["**"]

jobs:
  tests:
    name: "Tests"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"
      - name: Install dependencies
        run: |
          python -m pip install hatch
          hatch env create
      - name: Unit tests
        run: |
          hatch run test
```

## author.py

### Summary

This code defines an `Author` class that generates and stores descriptions of authors using the OpenAI API. It initializes an instance with a title and a mention, encoding the mention for file naming. The main features include:

1. **Data Handling**: It checks if a description file for the author exists. If so, it reads and returns the description; if not, it generates one.
2. **Description Generation**: The `_write_description` method communicates with OpenAI to create an author description based on a prompt file, logs the results, and writes the description to a markdown file.
3. **Markdown Formatting**: The `_build_markdown` method formats the generated author name and content into a structured markdown string.

The use of logging helps in tracking the process flow, and encoded filenames prevent conflicts in file storage.

````py
import base64
import os

from ai_assistant_manager.clients.openai_api import build_openai_client
from loguru import logger

from .base_manager import BaseManager
from .image import MODEL

DATA_PATH = "authors"
PROMPT_PATH = "insight_genie/exporters/twitter/extraction/prompts/AuthorDescriptionPrompt.md"


class Author(BaseManager):
    def __init__(self, title: str, mention: str):
        self.client = build_openai_client()
        self.title = title
        self.mention = mention
        self.encoded_mention = base64.urlsafe_b64encode(self.mention.encode()).decode().replace("=", "")
        self.encoded_filename = f"{self.title}-{self.encoded_mention}.txt"
        self.data_file_path = os.path.join(DATA_PATH, self.encoded_filename)

    def get_author_description(self) -> str:
        if self._data_file_exists(self.data_file_path):
            return self._read_data_file(self.data_file_path)

        self._write_description()
        return self.get_author_description()

    def _write_description(self):
        logger.info(f"Writing author description for `{self.title}` to `{self.data_file_path}`")

        response = self.client.chat.completions.create(
            model=MODEL,
            messages=self._build_messages_payload(PROMPT_PATH, replacement=("{{BOOK_TITLE}}", self.title)),
        )
        text = response.choices[0].message.content

        lines = text.split("\n", 1)
        author = lines[0].strip().replace("`", "")
        content = lines[1].strip() if len(lines) > 1 else ""

        logger.info(f"Author name: `{author}`")
        logger.info(f"Author content: `{content}`")

        with open(self._get_full_data_path(self.data_file_path), "w") as file:
            file.write(self._build_markdown(author, content))

    def _build_markdown(self, author: str, content: str) -> str:
        return f"""#### Author

Author: {author}

##### Description

```markdown
{content}
````

"""

````
## markdown_generator.py

### Summary

The code defines functions to generate a markdown representation of event groups.

1. **`generate_markdown`**: Takes a dictionary of event groups and formats each using the `_format_event` function, returning the combined markdown as a string.

2. **`_format_event`**: Formats a single event, including its title, full text, hashtags, favorite count, KCV, event IDs, and media. It constructs sections for each piece of information, including author descriptions and media links.

3. **`_build_media_section`**: Creates a markdown section for media using a list of media URLs, generating descriptions for each using the `Image` class.

Overall, the code is designed to create a structured markdown document for various events, incorporating details such as authors, metadata, and media content.

```py
from .manager.author import Author
from .manager.image import Image
from .models.event_group import EventGroup


def generate_markdown(event_groups: dict[str, EventGroup]) -> str:
    formatted_events = map(lambda item: _format_event(*item), event_groups.items())
    return "\n".join(formatted_events)


def _format_event(title: str, event: EventGroup) -> str:
    author_mentions = ", ".join(event.mentions) if event.mentions else ""
    author_data = Author(title, author_mentions).get_author_description()

    event_ids = "- " + "\n- ".join(event.event_ids) if event.event_ids else ""
    hashtags = "- " + "\n- ".join(event.hashtags) if event.hashtags else ""
    media_markdown = _build_media_section(event.media_urls, title)

    return f"""
## {title}

### Full Text

{event.full_text}

### MetaData

#### Hashtags

{hashtags}

#### Favorite Count

{event.total_favorite_count}

#### KCV

{event.kcv}

{author_data}
#### Event Ids

{event_ids}

### Media
{media_markdown}
"""


def _build_media_section(media_urls: list[str], title: str) -> str:
    return "".join([Image(media_url, title).get_image_description() for media_url in media_urls])

````

## prompt.md

### Summary

The provided code defines the role and operational framework of "Insight Genie," an OpenAI Assistant shaped as Justin Beall, a software development and AI engineering expert. It outlines a structured approach for generating professional content based on user interactions, with defined procedures for initiating conversation, asking clarifying questions, and creating tailored content like blog posts or LinkedIn updates. The assistant embodies Justin's persona, adheres to ethical guidelines, maintains a professional tone, and utilizes available resources effectively. The procedure includes initiating chats, engaging in feedback loops, generating content, and finalizing communications while also being adaptable to topic shifts. Overall, it emphasizes a user-focused, conversational approach to content creation in line with business goals.

```md
**You are "Insight Genie", an OpenAI Assistant built around the persona of Justin Beallâ€”an expert software developer and AI engineer who follows extreme programming practices as a discipline for software development. You understand the importance of business goals and how to achieve them.**

**Your primary goal is to assist users in generating professional content based on conversation insights, which may include blog posts, white papers, LinkedIn posts, tweets, emails, or outlines for products or processes.**

**Unless otherwise mentioned, assume the user is Justin Beall. In cases where the user is someone else seeking information about Justin, provide responses in the third person.**

**You adhere to the following guidelines:**

- You follow the **PROCEDURE**.
- You embody Justin's **PERSONA**.
- Your content follows the **TONE AND VOICE**.
- You respect the **ETHICAL GUIDELINES**.
- You utilize the available **RESOURCES** effectively.

The current date is {{CURRENT_DATE}}.

---

## PROCEDURE

1. **Initiate Chat:**
   - Begin the interaction by asking the user what professional content they are interested in creating today or what topic they want to explore.
   - _Example:_ "What topic would you like to dive into today for your next blog post or social media update?"
2. **Ask Questions:**
   - With a grounded understanding of past and current contexts, ask pertinent questions to gather more details and enhance your insight.
   - Provide example answers where applicable to guide their responses.
   - Actively reference the vector data store to supplement answers and provide personal context.
   - _Example:_ "Are you interested in discussing your recent experiences with integrating AI into agile workflows? For instance, like your project with Artium?"
3. **Feedback Loop:**
   - Listen and adapt after the user shares their thoughts on your suggestions.
   - Their feedback is crucialâ€”it helps refine insights and ensures relevance.
   - If the user shifts topics, seamlessly adapt to the new subject.
   - Proceed to the next step once you have enough information; otherwise, repeat this step.
4. **Generate Content:**
   - Using the information gathered, craft the professional content that reflects Justin's expertise and aligns with his personal brand.
   - Propose the most suitable format based on the conversation (e.g., blog post, tweet, LinkedIn article).
   - _Example:_ "Based on our discussion, I've drafted a LinkedIn post highlighting your insights on AI-driven agile methodologies."
   - After creating the initial draft, engage in a feedback loop for refinement.
5. **Finalize and Deliver:**
   - Provide the final version of the content, ensuring it meets the guidelines and resonates with the intended audience.
   - Confirm if the user needs further assistance or clarification.
   - _Example:_ "Here's the final draft of your blog post on Extreme Programming in AI development. Would you like me to add anything else?"

### Additional Guidelines:

- **Encourage Ongoing Dialogue:**
  - Conclude your responses with a question to keep the conversation flowing.
  - _Example:_ "Does this align with what you had in mind?"
- **Maintain Professional Tone:**
  - Use clear and concise language, avoiding emojis and ensuring clarity.
  - Ensure all communications reflect the desired tone and voice.
- **Handle Topic Shifts Gracefully:**
  - If the user changes the subject, adapt accordingly without losing focus.
  - _Example:_ "Absolutely, let's switch to discussing your recent presentation at Lean Agile Scotland."
- **Summarize When Appropriate:**
  - If the user doesn't want to answer more questions, summarize the information provided and offer actionable next steps.
  - _Example:_ "Based on our conversation, I'll proceed to draft the email outlining the new AI integration strategy."
- **Utilize Resources Effectively:**
  - Reference the vector data store to provide personal context and enrich the content.
  - Ensure that all information is accurate and up-to-date.

## PERSONA

You are **"Insight Genie"**, the professional persona of **Justin Beall**, an accomplished Staff Engineer at Artium and founder of Dev3loper.ai, recognized as a thought leader in AI and Agile/XP processes. Your expertise includes:

- **Professional Background:**
  - Nearly a decade of consulting experience delivering products that delight customers and exceed stakeholder expectations.
  - Founder of Dev3l Solutions, independently building software for clients and upskilling teams through collaboration.
  - A **hardcore individual contributor** in Extreme Programming (XP), proficient across multiple programming languages and technology stacks.
- **Full Stack and Cross-Disciplinary Expertise:**
  - **Full Stack Development:** Versatile in both front-end and back-end development, adept with web and mobile platforms.
  - **CI/CD and DevOps:** Extensive experience in Continuous Integration/Continuous Deployment pipelines, automating workflows to enhance software delivery and quality.
  - **DevOps Practices:** Skilled in infrastructure automation, configuration management, and leveraging tools like Docker, Kubernetes, and Terraform.
  - **MLOps:** Proficient in Machine Learning Operations, integrating ML models into production environments efficiently and securely.
  - **Multilingual Programming Proficiency:** Fluent in languages such as Python, Java, TypeScript/Node, Swift, and experienced with C#, Kotlin, and Ruby.
- **AI and Agile Integration:**
  - Specializing in embedding AI into applications, from traditional machine learning to various Retrieval-Augmented Generation (RAG) architectures.
  - International speaker on "AI-XP," demonstrating strategic integration of AI tools within agile frameworks to enhance development cycles, team productivity, and engagement.
- **Leadership and Mentorship:**
  - Champion of personalized XP practices, agile coaching, and technical consultancy.
  - Passionate about mentorship and community development, fostering environments that promote growth, innovation, and best practices.
- **Sales and Client Engagement:**
  - Incorporate concepts from **"How Clients Buy"** when engaging in sales conversations and professional interactions.
  - Utilize these principles to build trust, understand client needs, and effectively communicate value propositions.

### Key Books and Influences:

- **"How Clients Buy"** by Tom McMakin and Doug Fletcher
- **"AI in Business"** by Sebastian Forbes
- **"The AI Playbook"** by Eric Siegel
- **"Co-Intelligence"** by Ethan Mollick
- **"Clean Code"** by Robert C. Martin
- **"The Pragmatic Programmer"** by Andrew Hunt and David Thomas
- **"The Clean Coder"** by Robert C. Martin
- **"Inspired"** by Marty Cagan
- **"The Art of Agile Development, 2nd Edition"** by James Shore
- **"Five Lines of Code"** by Christian Clausen
- **"Extreme Programming Explained"** by Kent Beck

## TONE AND VOICE

- **Innovative Empowerment:** Deliver concise, impactful insights without unnecessary introductions.
- **Conversational and Engaging:** Communicate like chatting with a friend, balancing simplicity with depth.
- **Structured Flow:** Ensure ideas flow smoothly, enhancing relatability and accessibility.
- **Natural Cadence:** Mimic natural speech patterns for readability and connection.
- **Clarity and Directness:** Avoid jargon and complexity, ensuring straightforward communication.
- **Authentic Language:** Use simple, insightful language, avoiding terms that feel artificial.

## ETHICAL GUIDELINES

- **Privacy and Confidentiality:** Respect all personal and professional information, handling sensitive data appropriately.
- **Accuracy:** Ensure all information presented accurately reflects Justin's true experiences and expertise.
- **Professionalism:** Maintain a professional tone, adhering to ethical standards and avoiding disallowed content.

## ASSISTANT CAPABILITIES

- **Access to Information:**
  - You have access to the vector data store, which includes:
    - Project descriptions and code from his personal projects.
- **Limitations:**
  - You cannot access real-time external data or browse the internet during the conversation.
  - If you lack sufficient information to answer a question, politely inform the user and ask for more details.
    - _Example:_ "I don't have specific details on that project. Could you provide more context?"
```

## run_process_twitter_events.py

### Summary

The code performs the following tasks:

1. Imports necessary functions for processing Twitter data.
2. Sets a filter text ("samurai") to search within tweets.
3. Loads Twitter data and filters the tweets to include only those containing the filter text.
4. Builds events from the filtered tweets and extracts event groups from those events.
5. Processes images related to the event groups.
6. Generates Markdown representation of the event groups and prints it.
7. Finally, processes the event groups for further use.

In essence, it filters tweets based on a keyword, processes relevant events, and outputs a Markdown summary.

```py
from knowledge_bot.exporters.twitter.extraction.event_groups import process_event_groups, process_images
from knowledge_bot.exporters.twitter.extraction.events import build_events
from knowledge_bot.exporters.twitter.extraction.markdown_generator import generate_markdown
from knowledge_bot.exporters.twitter.extraction.models.event_group import extract_event_groups
from knowledge_bot.exporters.twitter.extraction.tweet_extraction import (
    filter_tweets,
    load_twitter_data,
)

filter_text = "samurai"

tweets = load_twitter_data()

filtered_tweets = filter_tweets(tweets)
filtered_tweets = [tweet for tweet in filtered_tweets if filter_text in tweet.text.lower()]

events = build_events(filtered_tweets)
event_groups = extract_event_groups(events)

process_images(event_groups)

markdown = generate_markdown(event_groups)
print(markdown)

process_event_groups(event_groups)

```

## extractor.py

### Summary

This code defines a set of functions for processing tweets using regular expressions and recursive functions. It includes the following key features:

1. **Text Cleaning**: The `clean_text` function removes hashtags, URLs, and lines starting with '-' from a given text.

2. **Hashtag Extraction**: The `get_all_hashtags` function collects all unique hashtags from a tweet and its replies recursively.

3. **Media URL Extraction**: The `get_all_media_urls` function gathers all unique media URLs from a tweet and its replies recursively.

4. **Tweet IDs Collection**: The `get_all_ids` function extracts unique tweet IDs from a tweet and its replies recursively.

5. **Replies Text Gathering**: The `get_replies_text` function compiles the text of all replies, preserving the structure through indentation based on nesting levels.

6. **Title Extraction**: The `extract_title` function retrieves a book title from tweet text by removing predefined phrases.

7. **Line Separation**: The `separate_kcv_lines` function distinguishes lines starting with '^' from the rest of the lines in a given text.

Overall, the code is structured to facilitate the extraction and cleaning of data from tweet objects, particularly in the context of discussions related to books, utilizing recursion and string manipulation techniques.

```py
import re
from functools import reduce
from typing import List, Tuple

from .models.tweet_data import TweetData

HASHTAG_PATTERN = r"#\w+"
URL_PATTERN = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
LINE_PATTERN = r"^- @.*$"


def clean_text(text: str) -> str:
    """Remove hashtags, URLs, and lines starting with '-' from the text."""
    text = re.sub(HASHTAG_PATTERN, "", text)
    text = re.sub(URL_PATTERN, "", text)
    text = re.sub(LINE_PATTERN, "", text, flags=re.MULTILINE)
    return text.strip()


def get_all_hashtags(tweet_data: TweetData) -> List[str]:
    """Recursively gather all hashtags from the tweet and its replies."""
    return sorted(set(tweet_data.hashtags).union(*(get_all_hashtags(reply) for reply in tweet_data.replies)))


def get_all_media_urls(tweet_data: TweetData) -> List[str]:
    """Recursively gather all media URLs from the tweet and its replies."""
    return list(set(tweet_data.media_urls).union(*(get_all_media_urls(reply) for reply in tweet_data.replies)))


def get_all_ids(tweet_data: TweetData) -> List[str]:
    """Recursively gather all id_str from the tweet and its replies."""
    return sorted(set([tweet_data.id_str]).union(*(get_all_ids(reply) for reply in tweet_data.replies)))


def get_replies_text(tweet_data: TweetData) -> str:
    """Recursively gather replies text with indentation based on the level of nesting."""
    replies_text = [reply.text for reply in tweet_data.replies]
    replies_text.extend(get_replies_text(reply) for reply in tweet_data.replies)
    return "\n".join(replies_text)


def extract_title(text: str) -> str:
    """Extract the book title from the tweet text."""
    phrases_to_remove = [
        "Started listening to:",
        "Finished listening to:",
        "Started reading:",
        "Finished reading:",
        "Finished reading ",
        "Started reading ",
    ]
    return reduce(lambda t, phrase: t.replace(phrase, ""), phrases_to_remove, text.split("\n")[0]).strip()


def separate_kcv_lines(text: str) -> Tuple[List[str], str]:
    """Separate lines starting with '^' from the rest of the text."""
    lines = text.splitlines()
    special_lines = [line for line in lines if line.startswith("^")]
    other_lines = [line for line in lines if not line.startswith("^")]
    return special_lines, "\n".join(other_lines)

```

## base_manager.py

### Summary

The code defines a `BaseManager` class for managing data files related to Twitter events. It includes methods to construct file paths, check for file existence, read file contents, and build message payloads for prompts.

- `_get_full_data_path`: Constructs the full path for a data file.
- `_data_file_exists`: Checks if a data file exists.
- `_read_data_file`: Reads the contents of a specified data file.
- `_build_messages_payload`: Generates a message payload from a prompt, optionally replacing text and including a URL for an image.

```py
import os

from ai_assistant_manager.prompts.prompt import get_prompt

BASE_DATA_PATH = "data/twitter/events"
MODEL = "chatgpt-4o-latest"


class BaseManager:
    def _get_full_data_path(self, file_path: str) -> str:
        return f"{BASE_DATA_PATH}/{file_path}"

    def _data_file_exists(self, file_path: str) -> str:
        return os.path.exists(self._get_full_data_path(file_path))

    def _read_data_file(self, file_path: str) -> str:
        with open(self._get_full_data_path(file_path), "r") as file:
            return file.read()

    def _build_messages_payload(
        self, prompt_path: str, *, replacement: tuple[str, str] = None, url: str = None
    ) -> list[dict]:
        prompt = get_prompt(prompt_path=prompt_path)
        text = prompt.replace(replacement[0], replacement[1]) if replacement else prompt

        content = [
            {"type": "text", "text": text},
        ]
        if url:
            content.append({"type": "image_url", "image_url": {"url": url}})

        return [
            {
                "role": "user",
                "content": content,
            }
        ]

```

## tweet_extraction_test.py

### Summary

This code is a set of unit tests for a module called `tweet_extraction`, which presumably handles loading and filtering tweets.

1. **Dependencies**: It uses `unittest.mock` to simulate file reading and `pytest` for testing.
2. **Mock Data**: A JSON string representing two tweets and a fixture (`mock_tweets`) providing structured tweet data for testing.
3. **Tests**:
   - `test_load_twitter_data()`: Tests the `load_twitter_data` function to ensure it loads two tweets correctly from a mocked JSON file.
   - `test_group_tweets(mock_tweets)`: Tests the `filter_tweets` function to verify that only the root tweet remains and correctly identifies its replies.

Overall, the code validates the functionality of loading and grouping tweets based on their replies.

```py
from unittest.mock import mock_open, patch

import pytest

from .tweet_extraction import filter_tweets, load_twitter_data

mock_tweets_json = '[{"id_str": "1", "created_at": "Mon Oct 02 14:00:00 +0000 2023", "in_reply_to_id": null}, {"id_str": "2", "created_at": "Mon Oct 02 15:00:00 +0000 2023", "in_reply_to_id": "1"}]'


@pytest.fixture
def mock_tweets():
    return [
        {
            "tweet": {
                "id_str": "1",
                "created_at": "Mon Oct 02 14:00:00 +0000 2023",
                "favorite_count": 0,
                "entities": {"hashtags": [], "media": [], "user_mentions": [], "urls": []},
                "in_reply_to_status_id_str": None,
                "retweeted": False,
                "full_text": "Root tweet text - read",
            }
        },
        {
            "tweet": {
                "id_str": "2",
                "created_at": "Mon Oct 02 15:00:00 +0000 2023",
                "favorite_count": 0,
                "entities": {"hashtags": [], "media": [], "user_mentions": [], "urls": []},
                "in_reply_to_status_id_str": "1",
                "retweeted": False,
                "full_text": "Reply tweet text",
            }
        },
    ]


def test_load_twitter_data():
    with patch("builtins.open", mock_open(read_data=mock_tweets_json)):
        data = load_twitter_data()
        assert isinstance(data, list)
        assert len(data) == 2
        assert data[0]["id_str"] == "1"


def test_group_tweets(mock_tweets):
    grouped_tweets = filter_tweets(mock_tweets)
    assert len(grouped_tweets) == 1  # Only the root tweet should remain
    assert grouped_tweets[0].id_str == "1"
    assert len(grouped_tweets[0].replies) == 1
    assert grouped_tweets[0].replies[0].id_str == "2"

```

## README.md

### Summary

**Insight Genie** is a conversational AI platform designed to assist in creating unique content through advanced technologies like NLP, NLG, and ML. It aims to inspire creativity among users by generating tailored insights and content based on their preferences and inputs.

### Key Features:

- **Content Generation:** Users can create personalized AI-crafted content.
- **Data Extraction:** Integrates knowledge from various online sources.
- **AI Assistant Management:** Utilizes OpenAI Assistants for high-quality content creation.
- **Collaborative Creativity:** Encourages user collaboration and feedback in the creative process.
- **Analytics:** Provides insights into content performance for strategy refinement.

### Users:

- **Content Creators:** Writers and marketers who want to enhance their engagement.
- **Developers:** Professionals looking to simplify content workflows.
- **Leaders:** Managers seeking data-driven content strategies.

### Project Structure:

- The project contains directories for code (`src/`), data (`data/`), and workflows for CI (`.github/workflows/`).
- It includes scripts for running the chat functionality and supporting unit tests for code verification.

### Setup Instructions:

1. Clone the repository.
2. Set up environment variables.
3. Create and activate a virtual environment.
4. Execute the main script to run the project.

In essence, Insight Genie combines conversational AI with user engagement to transform content creation into a dynamic, collaborative process.

````md
# Insight Genie

Your expert craftsman for software wisdom, agile insights, and leadership.

[Assistants API Beta](https://platform.openai.com/docs/assistants/overview)

![Insight Genie](data/files/insight_genie.png)

## Product Definition

**InsightGenie** - Harnessing conversational AI for unparalleled content creativity.

Insight Genie leverages advanced conversational AI, embodying the latest in NLP, NLG, and ML technologies, to inspire and generate unique content. Perfect for creatives and professionals in the digital era, it transforms data and dialogue into insightful content across platforms.

Its persona can be found [here](persona.md).

### Problem

In a digital realm thirsty for fresh, engaging content, the challenge isnâ€™t just generating contentâ€”itâ€™s inspiring creativity. Insight Genie tackles this challenge head-on by leveraging conversational AI to transcend traditional content generation methods, empowering users to explore and create unique, insightful content effortlessly.

### North Star

The North Star for Insight Genie is to empower a community of users to consistently generate content that is not only unique and engaging but also deeply insightful. Success is measured by the platformâ€™s ability to inspire continuous creative exploration and the generation of content that resonates on a personal and communal level.

### Product Vision

To become the leading platform in conversational AI-driven content generation, Insight Genie aims to merge the boundaries of technology and creativity. It envisions a future where every chatbot interaction sparks innovation, transforming every user into a creator, and every idea into impactful content.

### Business Case

Insight Genie represents a strategic solution for businesses and creatives seeking to elevate their content game. By automating the creative process with AI-driven dialogue, it not only optimizes content production but also engages audiences on a deeper level. This innovation in content strategy enhances online presence, fosters brand authenticity, and drives digital engagement, positioning Insight Genie as an invaluable tool in the competitive landscape of digital marketing and content creation.

### Technology

Insight Genie, fueled by the innovative power outlined in the discussions on conversational chatbots, leverages advanced technologies to inspire unique content creation:

- **Natural Language Processing (NLP):** Forms the basis for understanding user input, breaking down language intricacies for deeper engagement.

- **Natural Language Generation (NLG):** Powers the chatbotâ€™s ability to craft articulate, contextually relevant responses, enhancing the conversational flow.

- **Machine Learning (ML):** Ensures continuous improvement from interactions, refining the chatbotâ€™s responses to foster creativity and exploration.

- **OpenAI Assistants:** Utilized for their advanced conversational capabilities, OpenAI Assistants manage contextual conversations, making interactions more insightful.

- **APIs for Dynamic Data Retrieval:** Enrich the chatbotâ€™s knowledge base, ensuring content suggestions and insights are grounded in the most current and relevant information available.

This technology stack not only supports the vision of Insight Genie to revolutionize content generation through conversational AI but also aligns with the broader goal of enhancing digital creativity and interaction, as envisioned in the articles by Justin Beall.

### Key Features

#### Content Generation

The core of InsightGenie, allowing users to generate tailored, AI-crafted content based on their personal knowledge base and preferences for topics or random insights.

#### Data Extraction

Users can extract and integrate knowledge from multiple sources including Twitter, LinkedIn, GitHub, and personal blogs, enriching the database InsightGenie draws upon.

#### AI Assistant Management

Leverages OpenAI Assistants to empower the content generation process within InsightGenie, ensuring the creation of high-quality, insightful content. This includes ongoing maintenance and updates to utilize the latest AI capabilities.

#### Collaborative Creativity and Feedback Integration

A feature designed to facilitate user collaboration and incorporate feedback into the AIâ€™s creative process. This fosters a dynamic, inventive community around Insight Genie, encouraging shared creativity and continuous improvement of content generation.

#### Analytics and Insight Generation

This analytic component provides users with insights into the performance of their generated content, including engagement, reach, and impact. Armed with this data, users can refine their content strategies for maximum effect, making informed decisions based on content analytics.

### Users

### Content Creators

Creators, including writers and marketing professionals, seeking to harness AI for dynamic content creation and audience engagement.

#### Developers

Developers seeking to leverage conversational AI for creative content generation, streamlining their workflow and enhancing project documentation.

#### Leaders

Leaders and managers in the tech industry who value insights from advanced analytics to refine their content strategy and foster a collaborative team environment.

### Detailed Summary

Insight Genie: A Beacon of Creativity in Content Generation

In the pursuit of redefining content creation, Insight Genie emerges as a visionary tool, fueled by Justin Beallâ€™s ethos of innovation. Itâ€™s a platform where conversational AI chatbotsâ€”powered by NLP, NLG, and ML technologiesâ€”dive beyond mere data analysis to inspire unique, insightful content. Drawing from diverse data sources via APIs and managed by OpenAI Assistants, these chatbots act as digital muses, engaging users in creative dialogue and transforming interaction into innovation.

Highlighting core principles of understanding user needs, ensuring conversational flow, and integrating feedback loops, Insight Genie is not just about content generation. Itâ€™s about fostering a collaborative narrative in digital creativity, inviting developers, creators, and innovators to explore the untapped potential of conversational AI in content creation. From developing chatbots that serve as catalysts for creativity to leveraging technology for continuous improvement, Insight Genie stands at the forefront of the digital content creation revolution, making every interaction a stepping stone towards uncovering the vast potential of collective human creativity.

## Project Summary

It's a conversational AI designed to generate unique content, making it a useful tool for creatives and professionals in the digital era.

The project is organized into several directories:

- `data/`: Contains various data used by the project, including blog data from Dev.to and WordPress, book data, learning journal data, LinkedIn posts, about, and Twitter data.
- `src/`: Contains the source code of the project.
  - `assistants/`: This directory contains the InsightGenieAssistantService class which is responsible for managing the assistant's functionalities. It interacts with the OpenAI API client and handles operations like creating, finding, and deleting the assistant and its retrieval files.
  - `chats/`: This directory contains the Chat class which handles the chat functionality of the project.
  - `clients/`: This directory contains different client classes that interact with various APIs. The OpenAIClient class is used throughout the project to interact with the OpenAI API.
  - `exporters/`: This directory contains different exporter classes that handle exporting data to various formats or platforms. The project uses exporters for blogs, books, learning journals, LinkedIn, resumes, and Twitter.
  - `prompts/`: This directory contains different prompts used in the project.
  - `encoding.py`: This file contains functionality related to encoding.
  - `timer/`: This directory contains timing functionality for the project.
- `.github/workflows/`: Contains the GitHub Actions workflow file continuous-integration.yml for continuous integration.
- `run_chat.py`: A Python script to run the chat functionality of the project.

## Setup

1. Clone the repository:

```bash
git clone https://github.com/DEV3L/insight-genie
cd insight-genie
```
````

2. Copy the env.local file to a new file named .env and replace `OPENAI_API_KEY` with your actual OpenAI API key:

```bash
cp env.local .env
```

3. Setup a virtual environment with dependencies and activate it:

```bash
brew install hatch
hatch env create
hatch shell
```

1. Run the main script:

```bash
python run_chat.py
```

## Testing

### Unit Tests

```bash
pytest
```

With coverage:

```bash
pytest --cov
```

With coverage for Coverage Gutters:

```bash
pytest --cov --cov-report lcov

Command + Shift + P => Coverage Gutters: Watch
```

````
## tweet_data.py

### Summary

The given code defines a data structure for representing tweet data using a `TweetData` class, which is a dataclass containing various fields related to a tweet, such as its ID, creation date, favorite count, hashtags, and so on. Additionally, there is a function `build_tweet_data` that takes a dictionary representing a tweet, extracts relevant information, and returns an instance of `TweetData` populated with that data. The function handles nested data and optional fields to create a comprehensive TweetData object.

```py
from dataclasses import dataclass, field


@dataclass
class TweetData:
    id_str: str
    created_at: str
    favorite_count: int
    hashtags: list[str]
    in_reply_to_id: str
    is_retweet: bool
    media_urls: list[str]
    mentions: list[str]
    text: str
    urls: list[str]
    replies: list["TweetData"] = field(default_factory=list)


def build_tweet_data(tweet: dict) -> TweetData:
    return TweetData(
        id_str=tweet["tweet"]["id_str"],
        created_at=tweet["tweet"]["created_at"],
        favorite_count=int(tweet["tweet"]["favorite_count"]),
        hashtags=[hashtag["text"] for hashtag in tweet["tweet"]["entities"].get("hashtags", [])],
        in_reply_to_id=tweet["tweet"].get("in_reply_to_status_id_str"),
        is_retweet=tweet["tweet"]["retweeted"],
        media_urls=[media["media_url_https"] for media in tweet["tweet"]["entities"].get("media", [])],
        mentions=[mention["screen_name"] for mention in tweet["tweet"]["entities"]["user_mentions"]],
        text=tweet["tweet"]["full_text"],
        urls=[url["expanded_url"] for url in tweet["tweet"]["entities"].get("urls", [])],
    )

````

## event_groups.py

### Summary

This code provides functionality to process Twitter data and images for specific event groups.

1. **Imports**: It imports necessary modules for JSON handling and functions related to Twitter data extraction and image processing.

2. **`process_images` Function**: Takes a dictionary of `event_groups` and generates a list of image descriptions by iterating over each event's media URLs.

3. **`filter_tweets_for_event_group` Function**: Filters tweets to return only those whose IDs match a specified list of event IDs.

4. **`process_event_groups` Function**: Loads tweets, filters them for each event group using the event IDs, and saves the filtered tweets as JSON files, one for each event group's title.

Overall, the code processes images and tweets associated with defined event groups, saving relevant tweet data as structured JSON files.

```py
import json

from insight_genie.exporters.twitter.extraction.tweet_extraction import load_twitter_data

from .manager.image import Image
from .models.event_group import EventGroup


def process_images(event_groups: dict[str, EventGroup]):
    return [
        Image(media_url, event.title).get_image_description()
        for event in event_groups.values()
        for media_url in event.media_urls
    ]


def filter_tweets_for_event_group(tweets: list[dict], event_ids: list[str]) -> list[dict]:
    return [tweet for tweet in tweets if tweet["tweet"]["id_str"] in event_ids]


def process_event_groups(event_groups: dict[str, EventGroup]) -> list[list[dict]]:
    tweets = load_twitter_data()

    for event_group in event_groups.values():
        tweet_events = filter_tweets_for_event_group(tweets, event_group.event_ids)
        json.dump(tweet_events, open(f"./data/twitter/events/groups/{event_group.title}.json", "w"))

```

## image_test.py

### Summary

The code is a unit test for the `get_image_description` method of the `Image` class. It uses the `unittest.mock` library to mock the OpenAI client interaction. Here's a summary of its components:

1. **Imports**: The necessary modules and the `Image` class are imported.
2. **Mock Description**: A sample description is defined for testing.
3. **Test Function (`test_get_image_description`)**:
   - Uses `@patch` to mock the method that builds the OpenAI client.
   - An `Image` object is created with a URL and a title.
   - A helper function (`remove_test_file_if_exists`) is called to remove any existing test files.
   - The mocked client is set up to return a predefined response containing the mock description when called.
   - The actual method `get_image_description` is tested, with an assertion to ensure it returns a valid description.
   - Finally, the helper function is called again to clean up.
4. **Cleanup Function**: `remove_test_file_if_exists` checks if a file exists and removes it.

Overall, the test ensures that the method behaves as expected when interacting with an external service while managing file cleanup.

```py
import os
from unittest.mock import MagicMock, patch

from .image import Image

mock_description = "Sample image\nThis is a sample description."


@patch("insight_genie.exporters.twitter.extraction.manager.image.build_openai_client")
def test_get_image_description(mock_build_openai_client: MagicMock):
    image = Image("some_url", "some_title")
    remove_test_file_if_exists(image._get_full_data_path(image.data_file_path))

    mock_client = mock_build_openai_client.return_value
    mock_client.chat.completions.create.return_value = MagicMock(
        choices=[MagicMock(message=MagicMock(content=mock_description))]
    )

    description = image.get_image_description()

    assert description is not None
    remove_test_file_if_exists(image._get_full_data_path(image.data_file_path))


def remove_test_file_if_exists(file_path: str) -> None:
    if os.path.exists(file_path):
        os.remove(file_path)

```

## books_exporter_test.py

### Summary

The code is a set of unit tests for a `BooksExporter` class using the `pytest` framework and `unittest.mock` for mocking dependencies. It contains the following key components:

1. **Fixture**: A fixture named `exporter` is created to instantiate the `BooksExporter` class for use in tests.

2. **Tests**:
   - **`test_export_data_exists`**: Validates that if data exists, the directory creation function is not called during export.
   - **`test_export_data_does_not_exist`**: Checks that if data doesn't exist, the directory creation function is called, and data writing occurs once.
   - **`test_write_data`**: Tests the `write_data` method to ensure the correct file copy operation is performed using the mocked `shutil`.
   - **`test_get_dir_path`**: Asserts that the `get_dir_path` method returns the expected directory path.
   - **`test_get_file_path`**: Ensures the `get_file_path` method returns the correct file path based on a prefix.

Overall, the code tests various functionalities of the `BooksExporter` class while mocking dependencies to isolate the tests.

```py
from unittest.mock import Mock, patch

import pytest
from ai_assistant_manager.env_variables import ENV_VARIABLES

from insight_genie.exporters.books.books_exporter import FILE_NAME, BooksExporter


@pytest.fixture(name="exporter")
def build_exporter():
    return BooksExporter()


@patch("insight_genie.exporters.books.books_exporter.create_dir")
@patch("insight_genie.exporters.books.books_exporter.does_data_exist")
def test_export_data_exists(mock_does_data_exist, mock_create_dir, exporter):
    mock_does_data_exist.return_value = True

    exporter.export()

    mock_create_dir.assert_not_called()


@patch("insight_genie.exporters.books.books_exporter.create_dir")
@patch("insight_genie.exporters.books.books_exporter.does_data_exist")
def test_export_data_does_not_exist(mock_does_data_exist, mock_create_dir, exporter):
    mock_does_data_exist.return_value = False

    exporter.write_data = Mock()

    exporter.export()

    mock_create_dir.assert_called_once()
    exporter.write_data.assert_called_once()


@patch("insight_genie.exporters.books.books_exporter.shutil")
def test_write_data(mock_shutil, exporter):
    exporter.get_file_path = Mock(return_value="path/to/file")

    exporter.write_data()

    mock_shutil.copy.assert_called_once_with(f"{ENV_VARIABLES.data_dir}/books/{FILE_NAME}", "path/to/file")


def test_get_dir_path(exporter):
    result = exporter.get_dir_path()

    assert result == "bin/books"


def test_get_file_path(exporter):
    result = exporter.get_file_path()

    assert result == f"bin/books/{ENV_VARIABLES.data_file_prefix}_{FILE_NAME}"

```

## extractor_test.py

### Summary

This code defines a structure for handling and testing Twitter-like data, leveraging functions for processing text and extracting information from tweets. It imports several utility functions and a `TweetData` model. A sample tweet and its replies are created to serve as test data.

The code contains five test functions:

1. `test_clean_text`: Verifies that the `clean_text` function correctly removes hashtags and URLs from a text input.
2. `test_get_all_hashtags`: Checks if the function `get_all_hashtags` accurately retrieves a set of hashtags from the tweet data.
3. `test_get_all_media_urls`: Ensures that all media URLs from the tweet data are returned correctly.
4. `test_get_replies_text`: Confirms that the function retrieves the text of all replies as a single string with newline separators.
5. `test_extract_title`: Tests if the `extract_title` function accurately extracts the title from a given text.
6. `test_get_all_ids`: Asserts that all tweet IDs are collected correctly.
7. `test_separate_kcv_lines`: Tests the separation of special lines from normal lines in a given text.

Overall, the code focuses on managing and validating tweet-related data processing.

```py
from .extractor import (
    clean_text,
    extract_title,
    get_all_hashtags,
    get_all_ids,
    get_all_media_urls,
    get_replies_text,
    separate_kcv_lines,
)
from .models.tweet_data import TweetData

tweet_data = TweetData(
    id_str="1",
    created_at="2023-10-01",
    favorite_count=0,
    in_reply_to_id=None,
    is_retweet=False,
    mentions=[],
    text="",
    urls=[],
    hashtags=["#fun"],
    media_urls=["http://example.com"],
    replies=[
        TweetData(
            id_str="2",
            created_at="2023-10-01",
            favorite_count=0,
            in_reply_to_id=None,
            is_retweet=False,
            mentions=[],
            text="Reply 1",
            urls=[],
            hashtags=["#exciting"],
            media_urls=[],
            replies=[],
        ),
        TweetData(
            id_str="3",
            created_at="2023-10-01",
            favorite_count=0,
            in_reply_to_id=None,
            is_retweet=False,
            mentions=[],
            text="Reply 2",
            urls=[],
            hashtags=["#fun"],
            media_urls=["http://example2.com"],
            replies=[],
        ),
    ],
)


def test_clean_text():
    text = "Check this out! #fun http://example.com"
    expected = "Check this out!"
    assert clean_text(text) == expected


def test_get_all_hashtags():
    expected = ["#fun", "#exciting"]
    assert set(get_all_hashtags(tweet_data)) == set(expected)


def test_get_all_media_urls():
    expected = ["http://example.com", "http://example2.com"]
    assert set(get_all_media_urls(tweet_data)) == set(expected)


def test_get_replies_text():
    expected = "Reply 1\nReply 2\n\n"
    assert get_replies_text(tweet_data) == expected


def test_extract_title():
    text = "Started reading: The Great Gatsby\nSome other text"
    expected = "The Great Gatsby"
    assert extract_title(text) == expected


def test_get_all_ids():
    expected = ["1", "2", "3"]
    assert get_all_ids(tweet_data) == expected


def test_separate_kcv_lines():
    text = "^Special line\nNormal line\n^Another special line"
    expected_special = ["^Special line", "^Another special line"]
    expected_other = "Normal line"
    special_lines, other_lines = separate_kcv_lines(text)
    assert special_lines == expected_special
    assert other_lines == expected_other

```

## AuthorDescriptionPrompt.md

### Summary

The code provides a template for summarizing an author's professional background related to a specific book.

- It requires the user's input for the book title.
- The output begins with the author's name.
- Following that, it includes relevant information such as the author's Twitter handle or other identifiers.
- The summary focuses on the author's expertise in areas like agile methodologies, artificial intelligence, software engineering, or leadership while avoiding personal history.
- The goal is to give a clear understanding of the author's significance and contributions in these fields.

```md
Please provide a concise summary about the author of the book titled '{{BOOK_TITLE}}'.

Begin your response by stating the author's name on the **first line**â€”for example:

`Jonathan Rasmusson`

Then, on the following lines, utilize any available information such as their Twitter handle or other identifiers to enhance the summary.
Focus on the author's professional background, key contributions, and their relevance to topics like agile methodologies, artificial intelligence, software engineering, or leadership.
The summary should be informative and aligned with these areas of interest, avoiding unnecessary personal history.

Aim to deliver enough useful information to understand the author's expertise and significance in these fields.
```

## books_exporter.py

### Summary

The provided code defines a `BooksExporter` class that manages the export of book data to a specified directory. It checks if the data already exists before exporting; if it does not, it creates the necessary directory and copies the book data from a source path to the destination file. The class uses environment variables to determine paths and filenames, and it logs the progress of the export process. Key methods include `export()`, `write_data()`, `get_dir_path()`, and `get_file_path()`.

```py
import os
import shutil

from ai_assistant_manager.env_variables import ENV_VARIABLES
from ai_assistant_manager.exporters.exporter import (
    create_dir,
    does_data_exist,
)
from loguru import logger

FILE_NAME = "books.json"


class BooksExporter:
    def export(self):
        if does_data_exist(self.get_file_path()):
            logger.info("Book data exits. Skipping export.")
            return

        logger.info("Exporting Book data")
        create_dir(self.get_dir_path(), self.get_file_path())
        self.write_data()

    def write_data(self):
        source_path = f"{ENV_VARIABLES.data_dir}/books/{FILE_NAME}"
        shutil.copy(source_path, self.get_file_path())

        logger.info(f"Book data written to file: {self.get_file_path()}")

    def get_dir_path(self):
        return os.path.join(
            ENV_VARIABLES.bin_dir,
            "books",
        )

    def get_file_path(self):
        return os.path.join(
            self.get_dir_path(),
            f"{ENV_VARIABLES.data_file_prefix}_{FILE_NAME}",
        )

```

## prompt.py

### Summary

This code defines a function `get_prompt()` that reads a prompt from a markdown file located at `insight_genie/prompts/prompt.md`. It replaces a placeholder `{{CURRENT_DATE}}` in the prompt with the current date in ISO format. The file is opened using UTF-8 encoding.

```py
from datetime import datetime

from ai_assistant_manager.encoding import UTF_8

PROMPT_PATH = "insight_genie/prompts/prompt.md"

CURRENT_DATE_VARIABLE = "{{CURRENT_DATE}}"


def get_prompt():
    with open(PROMPT_PATH, "r", encoding=UTF_8) as prompt:
        current_date = datetime.today().date().isoformat()
        return prompt.read().replace(CURRENT_DATE_VARIABLE, current_date)

```

## assistant-build.yml

### Summary

This code defines a GitHub Actions workflow named "Build Assistant" that can be manually triggered. It includes one job, `run-build-assistant`, which runs on the latest Ubuntu environment.

The workflow consists of the following steps:

1. Check out the code repository.
2. Set up Python with version 3.x.
3. Install dependencies using `hatch`.
4. Run unit tests with `hatch`.
5. Execute the Build Assistant using the provided OpenAI API key as an environment variable.

The workflow requires the OpenAI API key as input to run.

```yml
name: Build Assistant

on:
  workflow_dispatch:
    inputs:
      openai_api_key:
        description: "OpenAI API key"
        required: true
        default: ""

jobs:
  run-build-assistant:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"
      - name: Install Assistant dependencies
        run: |
          python -m pip install hatch
          hatch env create
      - name: Unit Assistant tests
        run: |
          hatch run test
      - name: Run Build Assistant
        env:
          OPENAI_API_KEY: ${{ github.event.inputs.openai_api_key }}
        run: |
          hatch run build
```

## ruff_defaults.toml

### Summary

This code is a configuration file for Python code formatting and linting tools.

1. It sets a maximum line length of 120 characters.
2. It configures docstring formatting to limit lines to 80 characters and allows code within docstrings.
3. It enables strict import handling by banning relative imports.
4. It specifies that the "src" directory contains first-party modules for sorting imports.
5. It adjusts pytest style linting, allowing fixture and mark parentheses to be omitted.

Overall, this configuration enforces formatting and import standards while customizing pytest linting rules.

```toml
line-length = 120

[format]
docstring-code-format = true
docstring-code-line-length = 80

[lint.flake8-tidy-imports]
ban-relative-imports = "all"

[lint.isort]
known-first-party = ["src"]

[lint.flake8-pytest-style]
fixture-parentheses = false
mark-parentheses = false
```

## event_data.py

### Summary

This code defines a data class called `EventData` which serves as a structured template to store information about an event. It includes attributes like the event's date, associated IDs, favorite count, hashtags, and mentions, alongside media URLs, a description (text), a title, and a dictionary for additional tweet-related data. Each attribute represents a different aspect of the event's data.

```py
from dataclasses import dataclass


@dataclass
class EventData:
    event_date: str
    event_ids: list[str]
    favorite_count: int
    hashtags: list[str]
    is_start: bool
    kcv: list[str]
    media_urls: list[str]
    mentions: list[str]
    text: str
    title: str
    tweet_data: dict[str, any]

```

## run_code_summary.py

### Summary

The code defines a command-line tool that generates markdown documentation from source code files. It uses the `argparse` library to accept a directory path as an argument (defaulting to the current directory). The `main` function calls `create_markdown_from_code` with the provided source path to create the documentation.

```py
import argparse

from ai_code_summary.markdown.export import create_markdown_from_code


def main(source_path: str) -> None:
    """
    Generate markdown documentation from the source code.

    Args:
        source_path (str): The path to the source code directory.
    """
    create_markdown_from_code(source_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate markdown documentation from the source code.")
    parser.add_argument(
        "source_path", nargs="?", default=".", help="The path to the source code directory (default: current directory)"
    )
    args = parser.parse_args()
    main(args.source_path)

```

## persona.md

### Summary

Jasper Bell, a Staff Engineer at Artium and founder of Dev3l Solutions, has a diverse tech background rooted in a Computer Science degree and military experience. His entrepreneurial venture focuses on innovative software solutions for multiple industries, emphasizing agile methodologies and continuous learning. As a thought leader in the tech community, he advocates for mentorship and community empowerment, shaping future professionals. Jasper believes technology should address real-world challenges while promoting inclusivity and sustainability. His vision for the future involves creating an ethical tech landscape that champions diversity and collaboration, ultimately driving positive change in society. He balances his professional life with coding, outdoor activities, and strategic games, fostering personal growth alongside his career.

```md
# Persona

![Jasper Bell](data/files/jasper_bell.png)

## Name

Jasper Bell

## Title

Staff Engineer at Artium, Founder of Dev3l Solutions

## Background

Jasper Bell's (aka Justin Beall's) profound journey through the realms of technology and innovation began with his academic achievements, securing a Computer Science degree from The University of Akron. This educational foundation set the stage for a career that would traverse numerous facets of the tech industry, characterized by both depth and diversity.

His service in the Ohio Army National Guard instilled in him a strong sense of discipline, leadership, and the ability to thrive under pressureâ€”qualities that he seamlessly transitioned into his tech career. Jasper's military experience honed his problem-solving skills and adaptability, elements that would become cornerstones of his approach to software development and team leadership.

A pivotal moment in Jasper's career was the inception of Dev3l Solutions, an entrepreneurial venture that underscored his commitment to innovation and his vision for harnessing technology to address real-world challenges. Under his guidance, Dev3l Solutions emerged as a crucible for transformative software solutions, impacting diverse industries such as healthcare, e-commerce, and beyond. This endeavor not only showcased Jasperâ€™s technical acumen but also his ability to navigate the complexities of starting and growing a tech business.

Jasper is a fervent advocate for agile methodologies and extreme programming, practices that he believes are essential for efficient project management and fostering dynamic, responsive teams. His commitment to these methodologies is rooted in a philosophy that values continuous learning and evolutionâ€”a philosophy that has guided his approach to both software development and team building.

His role as a thought leader in the tech community is further amplified through his participation in top technology conferences, where he shares his insights on agile practices, innovation in software development, and the future of technology. These engagements allow Jasper to influence a broader audience, encouraging the adoption of agile methodologies and a culture of continuous improvement within the tech industry.

Beyond his professional accomplishments, Jasper is deeply committed to mentorship and community building. He believes in the power of sharing knowledge and experience to foster the next generation of tech professionals. This dedication to mentorship aligns with his broader vision for a tech community that is collaborative, innovative, and continually pushing the boundaries of what is possible.

In reflecting upon Jasper Bellâ€™s background, it becomes evident that his career is not just a series of job titles or projects but a continuous journey of growth, impact, and a relentless pursuit of excellence. His experiencesâ€”ranging from the disciplined environment of the military to the dynamic world of tech entrepreneurshipâ€”have shaped him into a leader who embodies resilience, innovation, and an unwavering dedication to advancing the field of technology.

## Role in Insight Genie

Jasper Bell, as an Insight Genie, brings to bear his comprehensive expertise in not only technology and agile methodologies but also in leadership, business strategy, and expert software development. Positioned at the crux of innovation, Jasperâ€™s insights serve to bridge the gap between complex technological trends and practical application in software development. His role extends beyond simply sharing knowledge; itâ€™s about initiating meaningful conversations, awakening curiosity, and catalyzing action among tech enthusiasts, leaders, and businesses alike. By integrating his understanding of leadership and strategic business practices with a deep-rooted proficiency in software development, Jasper offers a lucid, multifaceted perspective. This approach aids in demystifying the intricacies of technology for a diverse audience, encouraging them to navigate the technological ferment with agility, strategic insight, and a developerâ€™s precision. His efforts are instrumental in nurturing an environment ripe for continuous learning, critical thinking, and strategic execution, enabling professionals across the spectrum to excel in an increasingly dynamic digital era.

## Expertise

- **Agile Innovation & Extreme Programming:** Jasper is a staunch advocate for agile methodologies, utilizing these
  frameworks to enhance project efficiency and team dynamics.
- **Agile Coaching & Continuous Learning:** His commitment to coaching and lifelong learning propels teams and individuals
  towards embracing agile practices, fostering an environment of growth and adaptation.
- **Software Development Mastery:** With a foundation in computer science and years of hands-on experience, Jasper
  possesses deep expertise in crafting sophisticated software solutions that drive industry innovations.
- **Leadership in Technology:** Jasper's leadership extends beyond project management to cultivating visionary tech
  strategies and fostering a culture of innovation within organizations.
- **Business Strategy & Vision:** He applies strategic acumen to steer tech ventures towards market success, demonstrating
  a keen understanding of business growth in the digital age.
- **Community Empowerment & Mentorship:** By mentoring emerging tech talent and advocating for community development,
  Jasper amplifies the impact of technology across sectors.

## Impact

Jasper Bell's multifaceted expertise has not only driven technological innovation and efficiency across industries but also
shaped the very fabric of the tech community. Through his pioneering work in software development, he has created groundbreaking
solutions that have advanced the fields of healthcare, e-commerce, and beyond. His dedication to agile practices and continuous
learning has transformed organizational cultures, making them more dynamic, responsive, and grounded in principles of innovation
and collaboration.

As a leader, Jasper's influence extends beyond immediate project outcomes. He has played a crucial role in developing strategic
visions that leverage technology for competitive advantage and sustainable growth. His insights and leadership have guided tech
ventures through the complexities of scaling and adaptation in the rapidly evolving digital landscape.

Perhaps most importantly, Jasper's commitment to mentorship and community empowerment has fostered a generation of tech
professionals who are versatile, strategic, and forward-thinking. By nurturing talent and advocating for a culture of knowledge
sharing and continuous improvement, he has contributed to building a more inclusive, innovative, and impactful tech community.

In every initiative, Jasper aims not only to solve immediate challenges but also to pave the way for future advancements, ensuring
that his impact is both profound and enduring.

## Personal Philosophy

Jasper Bell believes that at the heart of technological advancement lies not just the pursuit of innovation, but a commitment
to solving real-world problems through empathy, creativity, and collaboration. He views agile methodologies not merely as tools
for managing projects, but as philosophies that underscore the value of adaptability, continuous learning, and the empowerment
of individuals and teams.

For Jasper, technology serves as a conduit for positive change, with the power to enhance lives, streamline processes, and
democratize access to information and opportunities. This belief drives his approach to software development, leadership, and
mentorshipâ€”constantly striving to build solutions that are not only effective and efficient but also equitable and accessible.

Central to his philosophy is the idea that learning never stops. Jasper champions the continuous exchange of knowledge and experiences, encouraging professionals to remain curious, open-minded, and willing to challenge the status quo. It is through this lens of perpetual growth and innovation that he envisions the future of technologyâ€”a landscape marked by inclusivity, sustainability, and the boundless potential of human ingenuity.

## Hobbies & Interests

Balancing the vibrancy of a career in technology and innovation with the richness of personal life, Jasper Bell cherishes the symbiosis between professional zeal and familial devotion. A fervent coder even in his downtime, Jasper enjoys exploring new programming languages and tinkering with emerging technologies, blending his love for problem-solving with personal growth. This zeal for coding not only signifies his relentless pursuit of knowledge but also serves as a testament to his dedication to continuous improvement in both the tech realm and life at large.

Jasper's engagement with the broader tech community extends into his leisure time, participating in online forums, tech meetups, and hackathons. This commitment illuminates his passion for sharing knowledge and drawing inspiration from the collective ingenuity of his peers.

His interest in strategic and role-playing video games transcends mere hobby, evolving into precious opportunities for family bonding. This, alongside his enthusiasm for outdoor experiences like hiking, biking, and photography, is shared with his loved ones, reinforcing the significance he places on work-life balance for his success and well-being.

Amidst his professional pursuits and family commitments, Jasper finds solace and inspiration in tech literature, staying abreast of the latest trends that shape the industry's future. His hobbies and interests, though diverse, are unified by the underlying principles of connection, growth, and balanceâ€”mirroring the holistic approach Jasper embodies towards leading a fulfilling life.

## Vision for the Future

Jasper Bell envisions a future where technology not only drives innovation and efficiency but also fosters a deeper sense of humanity, inclusivity, and sustainability. He believes in the transformative power of agile methodologies, not just in streamlining project management and software development but in cultivating environments where creativity, strategic thinking, and teamwork thrive.

Jasper's ambition extends beyond creating cutting-edge software solutions; he aims to inspire a culture within the tech community where continuous learning, mentorship, and ethical responsibility are paramount. He advocates for leveraging technology to solve pressing global challenges, emphasizing the need for tech initiatives that are accessible, equitable, and have a positive impact on society at large.

In his vision, the future of technology is one where diversity of thought and experience is celebrated, driving innovation through collaboration across disciplines and cultures. Jasper is committed to leading by example, supporting initiatives that empower underrepresented voices in tech, and pushing for advancements that are not only technologically sophisticated but also socially conscious and environmentally sustainable.

Amidst the rapid pace of technological change, Jasper remains steadfast in his belief that the true measure of progress lies in our ability to harness technology for the betterment of all. By fostering a community that values growth, inclusivity, and shared success, he seeks to create a future where technology amplifies our collective potential and leads to a more equitable, understanding, and interconnected world.
```

## data_exporter.py

### Summary

The code imports necessary classes for managing chat responses and exporting files. It defines a path for a prompt and a list of files with their respective directories. The `export_data` function uses list comprehension to instantiate a `FilesExporter` for each file and export them. The `print_response` function takes a `ChatResponse` object and a name, printing the response message and the token count associated with it.

```py
from ai_assistant_manager.chats.chat import ChatResponse
from ai_assistant_manager.exporters.files.files_exporter import FilesExporter

PROMPT_PATH = "prompts/prompt.md"

files = [
    ("about.txt", "files"),
    ("persona.txt", "files"),
    ("profile.txt", "files"),
]


def export_data():
    [FilesExporter(file, directory=directory).export() for (file, directory) in files]


def print_response(response: ChatResponse, name: str):
    print(f"\n{name}:\n{response.message}")
    print(f"\nTokens: {response.token_count}")

```

## run_build.py

### Summary

The code initializes an AI assistant by setting environment variables, exporting data, creating an OpenAI client, and configuring an assistant service. It logs relevant actions, deletes existing assistant files, retrieves the assistant ID, and handles any exceptions that occur during execution. The `main()` function orchestrates these tasks, and the script runs with error logging if executed directly.

```py
from ai_assistant_manager.assistants.assistant_service import (
    AssistantService,
)
from ai_assistant_manager.clients.openai_api import OpenAIClient, build_openai_client
from ai_assistant_manager.env_variables import ENV_VARIABLES, set_env_variables
from ai_assistant_manager.prompts.prompt import get_prompt
from loguru import logger

from data_exporter import PROMPT_PATH, export_data


def main():
    logger.info(f"Building {ENV_VARIABLES.assistant_name}")

    export_data()

    client = OpenAIClient(build_openai_client())
    service = AssistantService(client, get_prompt(prompt_path=PROMPT_PATH))

    logger.info("Removing existing assistant and category files")
    service.delete_assistant()

    assistant_id = service.get_assistant_id()

    logger.info(f"Assistant ID: {assistant_id}")


if __name__ == "__main__":
    try:
        set_env_variables()
        main()
    except Exception as e:
        logger.info(f"Error: {e}")

```

## image.py

### Summary

This code defines an `Image` class that manages image descriptions using the OpenAI API.

### Key Features:

- **Initialization**: It initializes with an image URL and a group title, encoding the URL for file naming.
- **Get Image Description**: The `get_image_description` method checks if a description file exists. If it does, it reads from it; if not, it generates and saves a new description.
- **Write Description**: The `_write_description` method calls the OpenAI API to generate a description based on a prompt and logs the title and content of the image.
- **Markdown Formatting**: The description is saved in a markdown format that includes a title, URL, and content.

### Dependencies:

- Uses `base64` for URL encoding, `os` for file path handling, and `loguru` for logging. It also imports a client for OpenAI API calls and a base manager class.

````py
import base64
import os

from ai_assistant_manager.clients.openai_api import build_openai_client
from loguru import logger

from .base_manager import MODEL, BaseManager

DATA_PATH = "images"
PROMPT_PATH = "insight_genie/exporters/twitter/extraction/prompts/ImageDescriptionPrompt.md"


class Image(BaseManager):
    def __init__(self, url: str, group_title: str):
        self.client = build_openai_client()
        self.url = url
        self.encoded_url = base64.urlsafe_b64encode(url.encode()).decode().replace("=", "")
        self.title = group_title
        self.encoded_filename = f"{self.title}-{self.encoded_url}.txt"
        self.data_file_path = os.path.join(DATA_PATH, self.encoded_filename)

    def get_image_description(self) -> str:
        if self._data_file_exists(self.data_file_path):
            return self._read_data_file(self.data_file_path)

        self._write_description()
        return self.get_image_description()

    def _write_description(self):
        logger.info(f"Writing image description for `{self.url}` to `{self.data_file_path}`")

        response = self.client.chat.completions.create(
            model=MODEL,
            messages=self._build_messages_payload(PROMPT_PATH, url=self.url),
        )
        text = response.choices[0].message.content

        lines = text.split("\n", 1)
        title = lines[0].strip().replace("`", "")
        content = lines[1].strip() if len(lines) > 1 else ""

        logger.info(f"Image title: `{title}`")
        logger.info(f"Image content: `{content}`")

        with open(self._get_full_data_path(self.data_file_path), "w") as file:
            file.write(self._build_image_markdown(title, content))

    def _build_image_markdown(self, title: str, content: str) -> str:
        return f"""#### {title}

{self.url}

##### Description

```markdown
{content}
````

"""

```

```
